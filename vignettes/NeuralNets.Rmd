---
title: "Parallelising Neural Nets - micro-project for OxWaSP module 4"
author: "Giuseppe Di Benedetto, Leon Law, Kaspar Martens, Marcin Mider"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
## Introduction
Our R package contains an implementation of a Neural Network for a classification problem with any arbitrary layers and hidden features. To improve performace the major bulk of the code has been written in C, and can potentially be run in parallel on CPUs.

## Neural Net Structure
Diagram below^[image taken from http://cs231n.github.io/neural-networks-1/ website] represents a structure of a neural net
![alt text](../figures/neural_net2.jpeg)
Input layer denotes the data $x^{(0)}$ - number of nodes in an input layer equals the dimensionality of the data. Each hidden layer comprises of units/nodes and each of these has its own bias term and weight vector. The units from each layer take the ouputs of the previous layer, compute the score by applying the linear transformation according to its weights and bias and output a non-linear transformation of the score. The output of the nodes from the final layer is interpreted as the class probability for the classification problem. In the picture above $x$ would be three dimensional. In the final picture the final layer has only one node, which is a setup for either regression or binary classification problem. 

## Pseudocode
Essentially, the algorithm for training a neural network the following:

* initialise weights and biases according to N(0,0.1)
* for ( i in 1 : num_epochs ) do:
    * for ((x,y) in train_data) do:
        * feedforward pass
        * backpropagation
        * update parameters
    * end do
* end do

## Feedforward pass
For simplicity, we will focus on one hidden layer only for a K-class classification problem.
Each node on an $l^{th}$ hidden layer takes the input $x^{(l-1)}$ and transforms it according to $x_i^{(l)}=\sigma(x^{(l-1)T} w_i^{(l)}+b_i^{(l)})$, where $w_i^{(l)}$ and $b_i^{(l)}$ are respectively the weight vector and the bias term of the $i^{th}$ unit in the $l^{th}$ hidden layer. Also $\sigma(z)=\frac{1}{1+exp(-z)}$. The output layer does almost the same, but uses softmax instead of sigmoid. The outputs are $P(y = k|x)$ for $k=1\dots K$ - class labels.

## Backpropagation
We compute the error terms for all the nodes in the neural net. We proceeed recursively over layers starting from the last one and moving backwards, i.e. for $l=L,(L-1),\dots,1$ (L being output layer), for each node $i$ in a given layer $l$ we compute: $\delta_i^{(l)}$ - the error term for a given node. These allow us to compute the gradients of the loss function (we call it R) with respect to the weights and the biases: $\frac{\partial R}{\partial w_{ij}^{(l)}}$ and $\frac{\partial R}{\partial b_{i}^{(l)}}$.

## Parameters' update
By moving weights and biases along the direction of the gradients computed in the backpropagation step above we are aiming to minimise the  (regularised) loss (taken to be a cross entropy in the classification scenario). We use the momentum update as well as regularisation. First update momentum for layer $l$: $v^{(l)} = \mu v^{(l)} - \alpha(\Delta w^{(l)}+\lambda w^{(l)})$ and then update the weights $w^{(l)} = w^{(l)} +v^{(l)}$. Then repeat for the biases. Perform these for the output and all the hidden layers.
