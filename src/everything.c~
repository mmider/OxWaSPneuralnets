
#include <stdio.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_rng.h>
#include <gsl/gsl_randist.h>
#include <gsl/gsl_blas.h>
#include <math.h>
#include <stdbool.h>
#include <R.h>
#include <time.h>

typedef struct parameters
{
  gsl_matrix** weights;
  gsl_vector** biases;
  double step_size;
  double penalty;
  double total_cost;
  
  int* layer_sizes;
  int num_layers;
  int batch_size;
  int nrow;
  int ncol;
  
  gsl_rng* r;

  void (*trans)(gsl_vector* x);
  void (*trans_final)(gsl_vector* x);

  void (*trans_prime)(gsl_vector* x, gsl_vector** ans);
  void (*trans_final_prime)(gsl_vector* x, gsl_vector** ans);

  double (*cost)(gsl_vector* x, int y);
  void (*cost_prime)(gsl_vector* x, int y, gsl_vector* ans);

}par;

typedef struct parameters_core
{
  gsl_vector** gradient_biases;
  gsl_matrix** gradient_weights;
  gsl_vector** z;
  gsl_vector** transf_x;
  gsl_vector** delta;
  double total_cost;

  double learning_rate;
  gsl_vector* x;
  int y;
  
} par_c;

void print_bias(gsl_vector* bias);

void print_weight(gsl_matrix* weight);

void sigmoid(gsl_vector* x);

void sigmoid_prime(gsl_vector* x, gsl_vector** ans);

void softmax(gsl_vector* x);

void softmax_prime(gsl_vector* probs, int y, gsl_vector* ans);

double softmax_cost(gsl_vector* probs, int y);

void init_bias_object(gsl_vector* vec[], int* layer_sizes, int n);

void init_weight_object(gsl_matrix* mat[], int* layer_sizes, int n);

void randomize_bias(gsl_vector* biases[],
		    int* layer_sizes,
		    int num_lay,
		    gsl_rng* r);

void randomize_weight(gsl_matrix* weights[],
		      int* layer_sizes,
		      int n,
		      gsl_rng* r);

void init_parameters_core(par_c* q, par* p);

void destroy_bias_obj(gsl_vector* biases[], int n);

void destroy_weight_obj(gsl_matrix* weights[], int n);


void destroy_parameters_core(par_c* q, par* p);

void NeuralNets(int* layer_sizes, int num_layers, gsl_vector* train_data[],
		gsl_vector* ys,int num_iterations, int batch_size,
		double step_size,gsl_matrix* output_weights[], gsl_vector* output_biases[],
		int nrow, int ncol, double penalty, double cost_hist[]);

void forward(par_c* q, par* p);
void backpropagation (par_c* q, par* p);
void StochGradDesc(gsl_vector* train_data[], gsl_vector* ys, par* p);
void squared_error_prime(gsl_vector* prediction, int y, gsl_vector* ans);
void vec_to_mat(gsl_vector* vec, gsl_matrix** ans);
double correct_guesses(gsl_vector* test_data[],
		       gsl_vector* ys, gsl_vector* biases[],
		       gsl_matrix* weights[], int nrow, int num_layers,
					      int * layer_sizes);
void data_to_gsl_vectors(double* input_array, int nrow, int ncol, gsl_vector* out[]);
void nn(double* train_data, double* ys, int* layer_sizes, int* num_layers, int* num_iterations,
	int* batch_size, double* step_size, int* nrow, int* ncol, double* penalty, double* output, double* output2);
gsl_vector* array_to_gsl_vector(double* input_array, int n);


void print_bias(gsl_vector* bias)
{
  int n = (*bias).size;
  printf("\nprinting bias vector:\n");
  for (int i =0; i < n; i++)
    printf("b(%d)=%g\n",i,gsl_vector_get(bias,i));
}

void print_weight(gsl_matrix* weight)
{
  int n = (*weight).size1;
  int k = (*weight).size2;
  printf("\nprinting weight vector:\n");
  for (int i = 0; i < n; i++)
    for (int j =0; j < k; j++)
      printf("m(%d,%d) = %g\n",i,j,gsl_matrix_get(weight,i,j));
}

void sigmoid(gsl_vector* x){
  // takes vector x, computes sigmoid(x) and stores IN PLACE of x
  int len = (*x).size;
  double temp;
  for (int i = 0; i < len; i++){
    temp = gsl_vector_get(x, i);
    gsl_vector_set(x,i, 1/(1+exp(-temp)));
  }
}

void sigmoid_prime(gsl_vector* x, gsl_vector** ans){
  // takes vector x, computes derivative of sigmoid(x) and RETURNS the vector
  int len = (*x).size;
  *ans = gsl_vector_alloc(len);

  double temp;
  for (int i = 0; i < len; i++){
    temp = gsl_vector_get(x,i);
    gsl_vector_set(*ans,i,1/(exp(temp)+2+exp(-temp)));
  }
}

void softmax(gsl_vector* x){
  // takes vector x, computes softmax(x) and stores IN PLACE of x
  int len = (*x).size;
  double max_score = gsl_vector_max(x);
  double temp_probs[len];
  double total = 0;
  for (int i = 0; i < len; i++){
    temp_probs[i] = exp(gsl_vector_get(x,i)-max_score);
    total += temp_probs[i];
  }
  for (int i = 0; i < len; i++){
    gsl_vector_set(x,i,temp_probs[i]/total);
  }
}

void softmax_prime(gsl_vector* probs, int y, gsl_vector* ans){
  gsl_vector_memcpy(ans, probs);
  gsl_vector_set(ans, y, gsl_vector_get(probs,y)-1);
}

double softmax_cost(gsl_vector* probs, int y){
  // Calculate loss (note: y must be coded from 0, ..., n_classes)
  double loss = -log(gsl_vector_get(probs, y));
  return(loss);
}

void init_bias_object(gsl_vector* vec[], int* layer_sizes, int n)
{
  // allocate memory for each layer in layer_sizes to store the biases of units
  // n is the number of layers-1
  for (int i = 0; i < n; i++){
    vec[i] = gsl_vector_calloc(layer_sizes[i]);
  }
}

void init_weight_object(gsl_matrix* mat[], int* layer_sizes,int n)
{
  // allocate memory for each layer in layer_sizes to store the weights of units
  // n is the number of layers
  for (int i = 0; i < n-1; i++){
    mat[i] = gsl_matrix_calloc(layer_sizes[i+1],layer_sizes[i]);
  }
}

void randomize_bias(gsl_vector* biases[],
		    int* layer_sizes,
		    int n,
		    gsl_rng* r)
{
  // set initial values of biases to N(0,0.1)
  for (int i = 0; i < n; i++)
    for (int j = 0; j < layer_sizes[i]; j++){
      gsl_vector_set(biases[i], j, gsl_ran_gaussian(r, 0.1));
    }
}

void randomize_weight(gsl_matrix* weights[],
		      int* layer_sizes,
		      int n,
		      gsl_rng* r)
{
  // set initial values of weights to N(0,0.1)
  for (int i = 0; i < n; i++)
    for (int j = 0; j < layer_sizes[i+1]; j++)
      for (int k = 0; k < layer_sizes[i]; k++)
	gsl_matrix_set(weights[i],j,k,gsl_ran_gaussian(r,0.1));
}

void destroy_bias_obj(gsl_vector* biases[], int n)
{
  // free memory
  for (int i = 0; i < n; i++)
    gsl_vector_free(biases[i]);
}

void destroy_weight_obj(gsl_matrix* weights[], int n)
{
  // free memory
  for (int i = 0; i < n; i++)
    gsl_matrix_free(weights[i]);
}

void init_parameters_core(par_c* q, par* p)
{
  int n = (*p).num_layers;
  int* ls = (*p).layer_sizes;
  gsl_vector* gradient_biases[n-1];
  gsl_matrix* gradient_weights[n-1];
  gsl_vector* z[n];
  gsl_vector* transf_x[n];
  gsl_vector* delta[n];
  
  init_bias_object(gradient_biases, ls+1,n-1);
  init_weight_object(gradient_weights, ls,n);
  init_bias_object(z, ls, n);
  init_bias_object(transf_x, ls, n);
  init_bias_object(delta, ls+1, n - 1);

  (*q).gradient_biases = gradient_biases;
  (*q).gradient_weights = gradient_weights;
  (*q).z = z;
  (*q).transf_x = transf_x;
  (*q).delta = delta;
}

void destroy_parameters_core(par_c* q, par* p)
{
  int n = (*p).num_layers;
  destroy_bias_obj((*q).gradient_biases,n - 1);
  destroy_weight_obj((*q).gradient_weights,n - 1);
  destroy_bias_obj((*q).z,n);
  destroy_bias_obj((*q).transf_x, n);
  destroy_bias_obj((*q).delta, n-1);
}

void data_to_gsl_vectors(double* input_array, int nrow, int ncol, gsl_vector* out[]){
	// Fill the matrix element-by-element
	for (int i=0; i<nrow; i++){
		out[i] = gsl_vector_alloc(ncol);
		for (int j=0; j<ncol; j++){
			gsl_vector_set(out[i], j, input_array[j*nrow + i]);
		}
	}
}


gsl_vector* array_to_gsl_vector(double* input_array, int n){
	// Create an empty matrix
	gsl_vector* out = gsl_vector_alloc(n);
	// Fill the matrix element-by-element
	for (int j=0; j<n; j++){
		gsl_vector_set(out, j, input_array[j]);
	}
	return(out);
}


void nn(double* train_data, double* ys, int* layer_sizes, int* num_layers, int* num_iterations,
	int* batch_size, double* step_size, int* nrow, int* ncol, double* penalty, double* output, double* output2)
{
   gsl_vector* data_vectors[*nrow];
   data_to_gsl_vectors(train_data, *nrow, *ncol, data_vectors);

   gsl_vector* y = array_to_gsl_vector(ys, *nrow);

   gsl_matrix* output_weights[*num_layers-1];
   gsl_vector* output_biases[*num_layers-1];

   init_bias_object(output_biases, (layer_sizes+1), *num_layers-1);
   init_weight_object(output_weights, layer_sizes, *num_layers);
   double cost_hist[*num_iterations];
   NeuralNets(layer_sizes,*num_layers,data_vectors,y, *num_iterations, *batch_size,
	      *step_size, output_weights, output_biases,*nrow,*ncol,*penalty, cost_hist);
   *output = correct_guesses(data_vectors, y, output_biases, output_weights, *nrow, *num_layers, layer_sizes);
   output2 = cost_hist;
}
		



void NeuralNets(int* layer_sizes, int num_layers, gsl_vector* train_data[],
		gsl_vector* ys, int num_iterations, int batch_size,
		double step_size, gsl_matrix* output_weights[], gsl_vector* output_biases[],
		int nrow, int ncol, double penalty, double cost_hist[]
		//int transformation_function, int final_transformation
		)
{
  /*
    Takes: layer_sizes -    each element of a vector specifices the number of units in a layer,
                            the first layer must be of size equal to dimensionality of x, the
			    last must be of number of possible categories y may take.
	   num_layers -     number of layers, additional layer for the original x-es must be
	                    counted (and also included in layer_sizes as specified above)
	   train_data -     array of pointers, which point to separate x observations
	   ys -             elements of vector give labels to corresponding observations
	   num_iterations - number of times the full sweep of Stochastic Gradient Descent
	                    should run when fitting the model
	   batch_size -     number of observations in a batch - each core will receive one batch
	   step_size -      currently fixed parameter for Stoch Grad Desc,
	                    TO DO will be changed in later versions
	   output_weigts -  fitted weights will go here
	   output_biases -  fitted biases will go here
	   nrow -           number of x observations
	   ncol -           dimensionality of x

    Fits neural network using backpropagation and stochastic gradient descent
    algorithms for arbitrary number of layers and units in layers. Currently supports
    only classification problems
  */

  // all auxiliary parameters are stored here:
  par p;
  // set up the random number generator
  const gsl_rng_type* T;
  gsl_rng* r;
  gsl_rng_env_setup();
  T = gsl_rng_default;
  r = gsl_rng_alloc(T);
  gsl_rng_set(r, time(NULL));

  // initialize a list of vectors with biases
  // and a list of matrices with weights
  // there are no biases in the first layer so start from layer_size + 1
  gsl_vector* biases[num_layers-1];
  gsl_matrix* weights[num_layers-1];
  init_bias_object(biases, (layer_sizes+1),num_layers-1);
  init_weight_object(weights, layer_sizes,num_layers);

  // set initial values
  randomize_bias(biases, (layer_sizes+1), num_layers-1, r);
  randomize_weight(weights, layer_sizes, num_layers-1, r);

  // set parameters that will be passed around:
  p.biases = biases;
  p.weights = weights;
  p.layer_sizes = layer_sizes;
  p.num_layers = num_layers;
  p.batch_size = batch_size;
  p.step_size = step_size;
  p.r = r;
  p.trans = sigmoid;
  p.trans_prime = sigmoid_prime;
  p.trans_final = softmax;
  p.trans_final_prime = sigmoid_prime;
  p.penalty = penalty;
  p.cost = softmax_cost;
  /*
  if (transformation_function == 0){
    p.trans = relu;
    p.trans_prime = relu_prime;
  }
  else{
    p.trans = sigmoid;
    p.trans_prime = sigmoid.prime;
  }
  if (final_transformation == 0){
    p.trans_final = softmax;
    p.trans_final_prime = softmax_prime;
  }
  else {
    p.trans_final = sigmoid;
    p.trans_final_prime = sigmoid.prime;
  }
  */
  p.cost_prime = softmax_prime;
  p.nrow = nrow;
  p.ncol = ncol;
  // perform Stochasstic Gradient Descents num_iterations many times
  
  for (int i = 0; i < num_iterations; i++){
    p.total_cost = 0.0;
    StochGradDesc(train_data, ys, &p);
    cost_hist[i] = p.total_cost;
    Rprintf("%g\n",p.total_cost);
  }

  // copy the results to the output matrices and vectors
  for (int i = 0; i < num_layers-1; i++){
    gsl_vector_memcpy(output_biases[i], p.biases[i]);
    gsl_matrix_memcpy(output_weights[i], p.weights[i]);
  }
} 


void forward(par_c* q, par* p) {
  /*
    Takes a single x and performs a single forward sweep for a given
    state of neural network. Stores the z=Wx+b for each layer in the
    z "history array" and stores sigmoid(Wx+b) (or possibly softmax(Wx+b)
    if softmax == true for the last layer) for each layer in the transf_x
  */
  
  // the first layer is sort of artificial, it is just defined for notational
  // convenience to contain the orignial x observation
  gsl_vector_memcpy((*q).transf_x[0],(*q).x);
  gsl_vector_memcpy((*q).z[0],(*q).x);
  
  for (int i = 0; i <(*p).num_layers-1; i++){
    // compute Wx
    gsl_blas_dgemv(CblasNoTrans, 1, (*p).weights[i], (*q).transf_x[i], 0, (*q).transf_x[i+1]);
    // update to z=Wx+b and store
    gsl_vector_add((*q).transf_x[i+1], (*p).biases[i]);
    gsl_vector_memcpy((*q).z[i+1],(*q).transf_x[i+1]);
    // update to sigmoid(z) or softmax(z)
    if (i == (*p).num_layers-2){
      (*q).total_cost += (*p).cost((*q).transf_x[i+1], (*q).y);
      (*p).trans_final((*q).transf_x[i+1]);
    }
    else
      (*p).trans((*q).transf_x[i+1]);
  }
}

void StochGradDesc(gsl_vector* train_data[], gsl_vector* ys, par* p){
  /*
    Takes the data: train_data is an array of pointers to vectors containing
                    separate x variables,
		    ys is a vector where in each row gives the label for a
		    corresponding x variable,
    and additional parameters stored in p,
    nrow - number of x observations
    ncol - dimension of each x observation
    
    performs one full sweep of Stochastic Gradient Descent:
    shuffles the data, splits into batches, (TO DO: sends each batch to separate core)
    performs forward propagation and backward propagation for each observation within
    batch, updates biases and weights vectors and repeats for as many batches as can
    be fit in the nrow without any repeats
  */

  // shuffle the data:
  // the data will be drawn in order specified by the consecutive
  // numbers from the indices vector
  int m = (*p).nrow;
  int indices[m];
  for (int i = 0; i < m; i++){
    indices[i] = i;
  }
  gsl_ran_shuffle((*p).r,indices, m, sizeof(int));

  int batch_number = m / (*p).batch_size;
  
  for (int i = 0; i < batch_number; i++){
    // SGD update for a batch, this will be done by each core separately:
    // ----------------------------
    
    // initialise parameters that will be shared in each core
    par_c q;
    q.total_cost = 0;
    // init_parameters_core(&q, p);
    int t = (*p).num_layers;
    int* ls = (*p).layer_sizes;
    gsl_vector* gradient_biases[t-1];
    gsl_matrix* gradient_weights[t-1];
    gsl_vector* z[t];
    gsl_vector* transf_x[t];
    gsl_vector* delta[t];
  
    init_bias_object(gradient_biases, ls+1,t-1);
    init_weight_object(gradient_weights, ls,t);
    init_bias_object(z, ls, t);
    init_bias_object(transf_x, ls, t);
    init_bias_object(delta, ls+1, t - 1);

    q.gradient_biases = gradient_biases;
    q.gradient_weights = gradient_weights;
    q.z = z;
    q.transf_x = transf_x;
    q.delta = delta;
    
    // initialise objects that will be passed around:
    // gradients, z - which stores z=Wx+b from each layer, and transf_x=sigmoid(z)
    // for each observation from the batch perform forward and backward sweep
    for (int j = 0; j < (*p).batch_size; j++){
      q.x = train_data[i * (*p).batch_size + j];
      q.y = (int) gsl_vector_get(ys,i * (*p).batch_size + j);
      forward(&q,p);
      backpropagation(&q,p);
    }
    (*p).total_cost += q.total_cost;
    // TO DO
    // at this point the cores should communicate to update weights and biases
    // so the update shuould really happen outside the loop
    // update weights and biases:
    for (int j = 0; j < (*p).num_layers-1; j++){
      q.learning_rate = -(*p).step_size/ (*p).batch_size;
      gsl_blas_daxpy(q.learning_rate, q.gradient_biases[j], (*p).biases[j]);
      gsl_matrix_scale(q.gradient_weights[j], q.learning_rate);
      // apply penalty
      gsl_matrix_scale((*p).weights[j],1-(*p).penalty * q.learning_rate);
      gsl_matrix_add((*p).weights[j], q.gradient_weights[j]);
    }
    destroy_parameters_core(&q,p);
  }
}


void backpropagation (par_c* q, par* p)
{
  /*
    Takes x observation and a corresponding label y, as well as z and transf_x from
    the most recent forward pass. Calculates deltas for each unit and updates
    the cumulative gradient of biases and weights of a given batch
  */
  // Output layer first
  gsl_vector* cp;
  gsl_vector* sp;
  if (false){
    cp = gsl_vector_alloc((*p).layer_sizes[(*p).num_layers-1]);
    (*p).cost_prime((*q).transf_x[(*p).num_layers-1],(*q).y, cp);
    (*p).trans_final_prime((*q).z[(*p).num_layers-1], &sp);
    gsl_vector_mul(cp,sp);
    gsl_vector_memcpy((*q).delta[(*p).num_layers - 2], cp);
    gsl_vector_free(cp);
    gsl_vector_free(sp);
  }
  else {
    (*p).cost_prime((*q).transf_x[(*p).num_layers-1],(*q).y,(*q).delta[(*p).num_layers-2]);
  }
  // For previous layers
  for (int l = (*p).num_layers - 2; l > 0; l--){
    (*p).trans_prime((*q).z[l], &sp);
    gsl_blas_dgemv(CblasTrans,1,(*p).weights[l],(*q).delta[l],0,(*q).delta[l-1]);
    gsl_vector_mul((*q).delta[l-1],sp);
    gsl_vector_free(sp);
  }
  // Computation of the error function derivatives
  gsl_matrix* delta_temp;
  gsl_matrix* a_temp;

  for (int l = 0; l < (*p).num_layers - 1; l++){
    gsl_vector_add((*q).gradient_biases[l],(*q).delta[l]);
    delta_temp = gsl_matrix_alloc(1,(*p).layer_sizes[l+1]);
    vec_to_mat((*q).delta[l], &delta_temp);
    vec_to_mat((*q).transf_x[l], &a_temp);

    gsl_blas_dgemm(CblasTrans,CblasNoTrans,1,delta_temp, a_temp,1,(*q).gradient_weights[l]);
    gsl_matrix_free(delta_temp);
    gsl_matrix_free(a_temp);
  }
}

void squared_error_prime(gsl_vector* prediction, int y, gsl_vector* ans)
{
  // calulates the derivative of a cost function and RETURNs vector with it
  
  gsl_vector_memcpy(ans,prediction);
  double temp = gsl_vector_get(ans, y);
  gsl_vector_set(ans, y, temp - 1);
}

void vec_to_mat(gsl_vector* vec, gsl_matrix** ans)
{
  int n = (*vec).size;
  *ans = gsl_matrix_alloc(1,n);
  for (int i = 0; i < n; i ++)
    gsl_matrix_set(*ans,0,i,gsl_vector_get(vec,i));
}


double correct_guesses(gsl_vector* test_data[],
		       gsl_vector* ys, gsl_vector* biases[],
		       gsl_matrix* weights[], int nrow, int num_layers,
		       int * layer_sizes)
{
  /*
  par p;
  par_c q;
  p.weights = weights;
  p.biases = biases;
  p.num_layers = num_layers;
  p.layer_sizes = layer_sizes;
  */  
  gsl_vector* z[num_layers];
  gsl_vector* transf_x[num_layers];

  init_bias_object(z, layer_sizes, num_layers);
  init_bias_object(transf_x, layer_sizes, num_layers);
  
  //q.z = z;
  //q.transf_x = transf_x;

  double total = 0.0;
  for (int i = 0; i < nrow; i++){
    gsl_vector* x = test_data[i];
    int y = (int) gsl_vector_get(ys,i);

    gsl_vector_memcpy(transf_x[0],x);
    gsl_vector_memcpy(z[0],x);
  
    for (int i = 0; i <num_layers-1; i++){
      // compute Wx
      gsl_blas_dgemv(CblasNoTrans, 1, weights[i], transf_x[i], 0, transf_x[i+1]);
      // update to z=Wx+b and store
      gsl_vector_add(transf_x[i+1], biases[i]);
      gsl_vector_memcpy(z[i+1],transf_x[i+1]);
      // update to sigmoid(z) or softmax(z)
      sigmoid(transf_x[i+1]);
    }
    int y_fitted = gsl_vector_max_index(transf_x[num_layers-1]);
    // Rprintf("%d, %d\n", y_fitted, y);
    total += (y==y_fitted);
  }
  return total / nrow;
}
